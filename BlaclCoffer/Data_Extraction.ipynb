{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05d61ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import requests\n",
    "input_df = pd.read_excel('Input.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8867c263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>blackassign0096</td>\n",
       "      <td>https://insights.blackcoffer.com/what-is-the-r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>blackassign0097</td>\n",
       "      <td>https://insights.blackcoffer.com/impact-of-cov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>blackassign0098</td>\n",
       "      <td>https://insights.blackcoffer.com/contribution-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>blackassign0099</td>\n",
       "      <td>https://insights.blackcoffer.com/how-covid-19-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>blackassign0100</td>\n",
       "      <td>https://insights.blackcoffer.com/how-will-covi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             URL_ID                                                URL\n",
       "0   blackassign0001  https://insights.blackcoffer.com/rising-it-cit...\n",
       "1   blackassign0002  https://insights.blackcoffer.com/rising-it-cit...\n",
       "2   blackassign0003  https://insights.blackcoffer.com/internet-dema...\n",
       "3   blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...\n",
       "4   blackassign0005  https://insights.blackcoffer.com/ott-platform-...\n",
       "..              ...                                                ...\n",
       "95  blackassign0096  https://insights.blackcoffer.com/what-is-the-r...\n",
       "96  blackassign0097  https://insights.blackcoffer.com/impact-of-cov...\n",
       "97  blackassign0098  https://insights.blackcoffer.com/contribution-...\n",
       "98  blackassign0099  https://insights.blackcoffer.com/how-covid-19-...\n",
       "99  blackassign0100  https://insights.blackcoffer.com/how-will-covi...\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cd4e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in input_df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57349daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c501503e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Rising IT cities and its impact on the economy, environment, infrastructure, and city life by the year 2040. - Blackcoffer Insights\n",
      "\n",
      "We have seen a huge development and dependence of people on technology in recent years. We have also seen the development of AI and ChatGPT in recent years. So it is a normal thing that we will become fully dependent on technology by 2040. Information technology will be a major power for all the developing nations. As a member of a developing nation, India is rapidly growing its IT base. It has also grown some IT cities which will be the major control centres for Information technology by 2040.\n",
      "Rising IT cities\n",
      "\n",
      "Noida:- Noida in Uttar Pradesh near New Delhi is an emerging IT sector now. Many large companies like Google, Microsoft, IBM, Infosys and others have set up their companies here. Noida has a market base of billions of dollars and is doing a great job of boosting the national economy. The establishment of so many software companies has made Noida an information technology hub.\n",
      "Gurgaon:- Gurgaon in Haryana is also an emerging IT hub. Many large companies like Google, Microsoft, IBM, Infosys and others have set up their companies here. Gurgaon has a market base of billions of dollars and is doing a great job of boosting the national economy.\n",
      "Bengaluru:- Bengaluru is called as the IT hub of India. It is also a smart city. Many large companies like Google, Microsoft, IBM, Infosys and others have set up their companies here. Bengaluru has a market base of billions of dollars and is doing a great job of boosting the national economy.\n",
      "\n",
      "Kolkata:- Kolkata in West Bengal is an emerging major IT hub. The new Kolkata i.e. Saltlake Sector  5, New town, Rajarhat area of Kolkata is a major IT hub. The government is giving the software companies land at almost free of cost to set up the companies there. Many large companies like Google, Microsoft, IBM, Infosys and others have set up their companies here. Kolkata has a market base of billions of dollars and is doing a great job of boosting the national economy.\n",
      "Impact on Economy\n",
      "There is a huge impact of the rising IT cities on our economy. Some of the effects are-\n",
      "\n",
      "Demand:- The rising IT cities will greatly help to boost our economy. These will create a huge demand for raw materials. The products when ready will be a huge demand for the people too.\n",
      "Supply:-– Supply means the fulfilment of demand. In a large and highly populous country like India, there is always a demand for finished products. If more IT cities do not develop, the companies cannot fulfil the needs and desires of the people of a populous country like India. As IT cities develop, more IT companies will come, which will supply more and more finished IT products to our people.\n",
      "Market: A market is a place where different economic agents like buyers and sellers interact with one another. In a populous country like India, there is a huge market. As IT cities will grow, more and more IT companies will come from across the world and more will the competition in the market increase. This will help consumers as they will get more and more differentiated products and the market will also run smoothly. A competitive market is always good and healthy. We can safely assume that our oligopoly market will surely tend to reach a perfectly competitive market by the year 2040.\n",
      "Revenue:- As the market increases, more revenue will be generated. Now at present, the IT revenue of India is 245 million dollars, 19 million dollars more than the financial year 2022. If IT cities grow, then more companies will invest which leads to an increase in the IT market which in turn generates more revenue in India. We can expect that the IT revenue of India will cross or nearly tend to reach 10 billion dollars by 2040.\n",
      "\n",
      "Impact on Environment\n",
      "The rising IT cities will create a huge impact on the environment, the maximum of which will be harmful effects. The impact of rising IT cities on the environment is-\n",
      "\n",
      "Deforestation:- There will be cutting of trees in huge numbers to make the building of the IT companies which will cause great harm to the environment. The cutting of trees on a large scale will also cause mass degradation of forests.\n",
      "More carbon footprint:- The IT companies will generate more carbon footprint in the atmosphere. South Asian countries including India are known for their lower carbon footprint. But if the IT sector grows this way then we will also be at the same pace of generation of carbon footprint by 2040.\n",
      "Death of birds:- The cell phone and mobile towers by the telecom companies caused the death of birds which caused a great imbalance in the ecosystem. The number of sparrows has been reduced due to this phenomenon. If this goes on we can see the extinction of many bird species by 2040.\n",
      "\n",
      "Impact on infrastructure\n",
      "There are many contributions of the IT cities on infrastructure.  They are-\n",
      "\n",
      "Transportation:- The rising IT cities need an excellent transport system for the supply of raw materials and delivery of the finished products into the market. So the transportation system develops in that area. So we have an excellent transport system by 2040.\n",
      "Need for a public transport system:- There is a need for a public transport system in the IT cities. As the IT cities are a source of employment and a huge population reside in these areas, there is an adequate need for public transport systems like buses, taxis etc. We hope that it will be improved by 2040.\n",
      "Water supply:- As a huge number of people reside in the IT cities there is a need for adequate water supply to fulfil the needs of people as well as for industries. This will help us to find many new methods of water supply and conservation by 2040.\n",
      "Electricity:- Electric supply is the lifeline of the sector. Without an electric supply, no machines will run and not even the IT cities will flourish. If the IT cities flourish this way, we going to have an excellent electric supply by 2040.\n",
      "Healthcare:- As a large number of people reside in IT cities, there is a need for proper health infrastructure and healthcare facilities for the people. So with the growth of IT cities, our healthcare system will also improve by 2040.\n",
      "Education:- Education is the primary key or core of any nation. There must be proper education and training centres in those IT cities to fulfil the people’s demands.  So with the growth of IT cities, the education system will also develop by 2040. Our education is also going to be skill-oriented.\n",
      "\n",
      "Impact on city life\n",
      "With the growth of IT cities, more people will get jobs and will earn more. So the purchasing power of the people will increase. People will lead a better lifestyle. They will buy things of good brand value. The tastes and preferences of people will also change. The human development index is going to increase. People will buy good quality food and good quality cars. So the food, automobile and many other industries are going to increase. So there will be a huge impact on city life by 2040.\n",
      "Blackcoffer Insights 47: Arka Mukhopadhyay, West Bengal University Of Animal And Fishery Sciences \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def analyze_structure(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "    title_tag = soup.find('title')\n",
    "    if title_tag:\n",
    "        print(\"Title:\", title_tag.get_text())\n",
    "    \n",
    "    div_tag = soup.find('div', class_='td-post-content tagdiv-type')\n",
    "\n",
    "    if div_tag:\n",
    "        print(div_tag.get_text())\n",
    "\n",
    "url_to_analyze = input_df.iloc[0]['URL']\n",
    "analyze_structure(url_to_analyze)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d722cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_structure(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    title_tag = soup.find('title')\n",
    "    title = title_tag.get_text() if title_tag else \"No title found\"\n",
    "    \n",
    "    div_tag = soup.find('div', class_='td-post-content tagdiv-type')\n",
    "    article_text = div_tag.get_text() if div_tag else \"No article text found\"\n",
    "    \n",
    "    return title, article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30d0243d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed URL_ID blackassign0001. Data saved to blackassign0001_output.txt\n",
      "Processed URL_ID blackassign0002. Data saved to blackassign0002_output.txt\n",
      "Processed URL_ID blackassign0003. Data saved to blackassign0003_output.txt\n",
      "Processed URL_ID blackassign0004. Data saved to blackassign0004_output.txt\n",
      "Processed URL_ID blackassign0005. Data saved to blackassign0005_output.txt\n",
      "Processed URL_ID blackassign0006. Data saved to blackassign0006_output.txt\n",
      "Processed URL_ID blackassign0007. Data saved to blackassign0007_output.txt\n",
      "Processed URL_ID blackassign0008. Data saved to blackassign0008_output.txt\n",
      "Processed URL_ID blackassign0009. Data saved to blackassign0009_output.txt\n",
      "Processed URL_ID blackassign0010. Data saved to blackassign0010_output.txt\n",
      "Processed URL_ID blackassign0011. Data saved to blackassign0011_output.txt\n",
      "Processed URL_ID blackassign0012. Data saved to blackassign0012_output.txt\n",
      "Processed URL_ID blackassign0013. Data saved to blackassign0013_output.txt\n",
      "Processed URL_ID blackassign0014. Data saved to blackassign0014_output.txt\n",
      "Processed URL_ID blackassign0015. Data saved to blackassign0015_output.txt\n",
      "Processed URL_ID blackassign0016. Data saved to blackassign0016_output.txt\n",
      "Processed URL_ID blackassign0017. Data saved to blackassign0017_output.txt\n",
      "Processed URL_ID blackassign0018. Data saved to blackassign0018_output.txt\n",
      "Processed URL_ID blackassign0019. Data saved to blackassign0019_output.txt\n",
      "Processed URL_ID blackassign0020. Data saved to blackassign0020_output.txt\n",
      "Processed URL_ID blackassign0021. Data saved to blackassign0021_output.txt\n",
      "Processed URL_ID blackassign0022. Data saved to blackassign0022_output.txt\n",
      "Processed URL_ID blackassign0023. Data saved to blackassign0023_output.txt\n",
      "Processed URL_ID blackassign0024. Data saved to blackassign0024_output.txt\n",
      "Processed URL_ID blackassign0025. Data saved to blackassign0025_output.txt\n",
      "Processed URL_ID blackassign0026. Data saved to blackassign0026_output.txt\n",
      "Processed URL_ID blackassign0027. Data saved to blackassign0027_output.txt\n",
      "Processed URL_ID blackassign0028. Data saved to blackassign0028_output.txt\n",
      "Processed URL_ID blackassign0029. Data saved to blackassign0029_output.txt\n",
      "Processed URL_ID blackassign0030. Data saved to blackassign0030_output.txt\n",
      "Processed URL_ID blackassign0031. Data saved to blackassign0031_output.txt\n",
      "Processed URL_ID blackassign0032. Data saved to blackassign0032_output.txt\n",
      "Processed URL_ID blackassign0033. Data saved to blackassign0033_output.txt\n",
      "Processed URL_ID blackassign0034. Data saved to blackassign0034_output.txt\n",
      "Processed URL_ID blackassign0035. Data saved to blackassign0035_output.txt\n",
      "Processed URL_ID blackassign0036. Data saved to blackassign0036_output.txt\n",
      "Processed URL_ID blackassign0037. Data saved to blackassign0037_output.txt\n",
      "Processed URL_ID blackassign0038. Data saved to blackassign0038_output.txt\n",
      "Processed URL_ID blackassign0039. Data saved to blackassign0039_output.txt\n",
      "Processed URL_ID blackassign0040. Data saved to blackassign0040_output.txt\n",
      "Processed URL_ID blackassign0041. Data saved to blackassign0041_output.txt\n",
      "Processed URL_ID blackassign0042. Data saved to blackassign0042_output.txt\n",
      "Processed URL_ID blackassign0043. Data saved to blackassign0043_output.txt\n",
      "Processed URL_ID blackassign0044. Data saved to blackassign0044_output.txt\n",
      "Processed URL_ID blackassign0045. Data saved to blackassign0045_output.txt\n",
      "Processed URL_ID blackassign0046. Data saved to blackassign0046_output.txt\n",
      "Processed URL_ID blackassign0047. Data saved to blackassign0047_output.txt\n",
      "Processed URL_ID blackassign0048. Data saved to blackassign0048_output.txt\n",
      "Processed URL_ID blackassign0049. Data saved to blackassign0049_output.txt\n",
      "Processed URL_ID blackassign0050. Data saved to blackassign0050_output.txt\n",
      "Processed URL_ID blackassign0051. Data saved to blackassign0051_output.txt\n",
      "Processed URL_ID blackassign0052. Data saved to blackassign0052_output.txt\n",
      "Processed URL_ID blackassign0053. Data saved to blackassign0053_output.txt\n",
      "Processed URL_ID blackassign0054. Data saved to blackassign0054_output.txt\n",
      "Processed URL_ID blackassign0055. Data saved to blackassign0055_output.txt\n",
      "Processed URL_ID blackassign0056. Data saved to blackassign0056_output.txt\n",
      "Processed URL_ID blackassign0057. Data saved to blackassign0057_output.txt\n",
      "Processed URL_ID blackassign0058. Data saved to blackassign0058_output.txt\n",
      "Processed URL_ID blackassign0059. Data saved to blackassign0059_output.txt\n",
      "Processed URL_ID blackassign0060. Data saved to blackassign0060_output.txt\n",
      "Processed URL_ID blackassign0061. Data saved to blackassign0061_output.txt\n",
      "Processed URL_ID blackassign0062. Data saved to blackassign0062_output.txt\n",
      "Processed URL_ID blackassign0063. Data saved to blackassign0063_output.txt\n",
      "Processed URL_ID blackassign0064. Data saved to blackassign0064_output.txt\n",
      "Processed URL_ID blackassign0065. Data saved to blackassign0065_output.txt\n",
      "Processed URL_ID blackassign0066. Data saved to blackassign0066_output.txt\n",
      "Processed URL_ID blackassign0067. Data saved to blackassign0067_output.txt\n",
      "Processed URL_ID blackassign0068. Data saved to blackassign0068_output.txt\n",
      "Processed URL_ID blackassign0069. Data saved to blackassign0069_output.txt\n",
      "Processed URL_ID blackassign0070. Data saved to blackassign0070_output.txt\n",
      "Processed URL_ID blackassign0071. Data saved to blackassign0071_output.txt\n",
      "Processed URL_ID blackassign0072. Data saved to blackassign0072_output.txt\n",
      "Processed URL_ID blackassign0073. Data saved to blackassign0073_output.txt\n",
      "Processed URL_ID blackassign0074. Data saved to blackassign0074_output.txt\n",
      "Processed URL_ID blackassign0075. Data saved to blackassign0075_output.txt\n",
      "Processed URL_ID blackassign0076. Data saved to blackassign0076_output.txt\n",
      "Processed URL_ID blackassign0077. Data saved to blackassign0077_output.txt\n",
      "Processed URL_ID blackassign0078. Data saved to blackassign0078_output.txt\n",
      "Processed URL_ID blackassign0079. Data saved to blackassign0079_output.txt\n",
      "Processed URL_ID blackassign0080. Data saved to blackassign0080_output.txt\n",
      "Processed URL_ID blackassign0081. Data saved to blackassign0081_output.txt\n",
      "Processed URL_ID blackassign0082. Data saved to blackassign0082_output.txt\n",
      "Processed URL_ID blackassign0083. Data saved to blackassign0083_output.txt\n",
      "Processed URL_ID blackassign0084. Data saved to blackassign0084_output.txt\n",
      "Processed URL_ID blackassign0085. Data saved to blackassign0085_output.txt\n",
      "Processed URL_ID blackassign0086. Data saved to blackassign0086_output.txt\n",
      "Processed URL_ID blackassign0087. Data saved to blackassign0087_output.txt\n",
      "Processed URL_ID blackassign0088. Data saved to blackassign0088_output.txt\n",
      "Processed URL_ID blackassign0089. Data saved to blackassign0089_output.txt\n",
      "Processed URL_ID blackassign0090. Data saved to blackassign0090_output.txt\n",
      "Processed URL_ID blackassign0091. Data saved to blackassign0091_output.txt\n",
      "Processed URL_ID blackassign0092. Data saved to blackassign0092_output.txt\n",
      "Processed URL_ID blackassign0093. Data saved to blackassign0093_output.txt\n",
      "Processed URL_ID blackassign0094. Data saved to blackassign0094_output.txt\n",
      "Processed URL_ID blackassign0095. Data saved to blackassign0095_output.txt\n",
      "Processed URL_ID blackassign0096. Data saved to blackassign0096_output.txt\n",
      "Processed URL_ID blackassign0097. Data saved to blackassign0097_output.txt\n",
      "Processed URL_ID blackassign0098. Data saved to blackassign0098_output.txt\n",
      "Processed URL_ID blackassign0099. Data saved to blackassign0099_output.txt\n",
      "Processed URL_ID blackassign0100. Data saved to blackassign0100_output.txt\n"
     ]
    }
   ],
   "source": [
    "for index, row in input_df.iterrows():\n",
    "    url = row['URL']\n",
    "    url_id = row['URL_ID']\n",
    "    \n",
    "    # Extract title and article text\n",
    "    title, article_text = analyze_structure(url)\n",
    "    \n",
    "    # Save the data to separate .txt files\n",
    "    output_file_path = f\"{url_id}_output.txt\"\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(f\"Title: {title}\\n\\n\")\n",
    "        file.write(f\"Article Text:\\n{article_text}\")\n",
    "\n",
    "    print(f\"Processed URL_ID {url_id}. Data saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "111ca4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No article text found for URL_ID 14, URL: URL not found\n",
      "Warning: No article text found for URL_ID 20, URL: URL not found\n",
      "Warning: No article text found for URL_ID 29, URL: URL not found\n",
      "Warning: No article text found for URL_ID 36, URL: URL not found\n",
      "Warning: No article text found for URL_ID 43, URL: URL not found\n",
      "Warning: No article text found for URL_ID 49, URL: URL not found\n",
      "Warning: No article text found for URL_ID 83, URL: URL not found\n",
      "Warning: No article text found for URL_ID 84, URL: URL not found\n",
      "Warning: No article text found for URL_ID 92, URL: URL not found\n",
      "Warning: No article text found for URL_ID 99, URL: URL not found\n",
      "Warning: No article text found for URL_ID 100, URL: URL not found\n",
      "Filtering completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Directory where the text files are saved\n",
    "txt_files_directory = r'C:\\Users\\dell\\Downloads\\Python\\Blackcoffer'\n",
    "\n",
    "input_df = pd.read_excel('Input.xlsx')\n",
    "\n",
    "# Create a dictionary to map URL_ID to URL\n",
    "url_dict = dict(zip(input_df['URL_ID'], input_df['URL']))\n",
    "\n",
    "# Iterate through each .txt file\n",
    "for filename in os.listdir(txt_files_directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(txt_files_directory, filename)\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            file_content = file.read()\n",
    "\n",
    "            # Extract URL_ID using regular expression\n",
    "            match = re.search(r'(\\d+)_output.txt', filename)\n",
    "            if match:\n",
    "                url_id = int(match.group(1))\n",
    "                url = url_dict.get(url_id, \"URL not found\")\n",
    "\n",
    "                if \"No title found\" in file_content:\n",
    "                    print(f\"Warning: No title found for URL_ID {url_id}, URL: {url}\")\n",
    "\n",
    "                if \"No article text found\" in file_content:\n",
    "                    print(f\"Warning: No article text found for URL_ID {url_id}, URL: {url}\")\n",
    "            else:\n",
    "                print(f\"Invalid filename format: {filename}\")\n",
    "\n",
    "print(\"Filtering completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e836e7e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
