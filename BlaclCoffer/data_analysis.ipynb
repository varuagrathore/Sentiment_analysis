{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c43f28d-ad4f-41b3-8c80-ad199b3b558b",
   "metadata": {},
   "source": [
    "## Set Up Libraries, Paths, and Download NLTK Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc6e31ce-f85f-4d49-bdfa-0567bcd21a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "\n",
    "#  paths\n",
    "ARTICLES_FOLDER_PATH = r'C:\\Users\\dell\\OneDrive\\Sem5\\artilce\\articles'\n",
    "OUTPUT_FILE_PATH = 'Output Data.xlsx'\n",
    "POSITIVE_WORDS_PATH = r'C:\\Users\\dell\\OneDrive\\Sem5\\artilce\\MasterDictionary\\positive-words.txt'\n",
    "NEGATIVE_WORDS_PATH = r'C:\\Users\\dell\\OneDrive\\Sem5\\artilce\\MasterDictionary\\negative-words.txt'\n",
    "CUSTOM_STOPWORDS_FOLDER_PATH = r'C:\\Users\\dell\\OneDrive\\Sem5\\artilce\\StopWords'\n",
    "\n",
    "#  NLTK data is downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "print(\"Setup complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72716907-5763-4b2b-91b1-accecc4af588",
   "metadata": {},
   "source": [
    "##  Load Stop Words and Custom Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04c53294-6e45-45c9-8f31-f08b8b0df7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total stop words loaded: 12948\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load custom stop words\n",
    "for filename in os.listdir(CUSTOM_STOPWORDS_FOLDER_PATH):\n",
    "    file_path = os.path.join(CUSTOM_STOPWORDS_FOLDER_PATH, filename)\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        custom_stop_words = file.read().splitlines()\n",
    "        stop_words.update(custom_stop_words)\n",
    "\n",
    "print(f\"Total stop words loaded: {len(stop_words)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9187dab5-c39a-4496-97f6-a548101882eb",
   "metadata": {},
   "source": [
    "## Load Positive and Negative Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02623a1c-46a9-4780-b788-384a0bbef9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive words loaded: 2006\n",
      "Negative words loaded: 4783\n"
     ]
    }
   ],
   "source": [
    "# Load positive words\n",
    "with open(POSITIVE_WORDS_PATH, 'r') as file:\n",
    "    positive_words = set(file.read().splitlines())\n",
    "\n",
    "# Load negative words\n",
    "with open(NEGATIVE_WORDS_PATH, 'r') as file:\n",
    "    negative_words = set(file.read().splitlines())\n",
    "\n",
    "print(f\"Positive words loaded: {len(positive_words)}\")\n",
    "print(f\"Negative words loaded: {len(negative_words)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cee1b0-3c41-4189-845a-f9314bb1104c",
   "metadata": {},
   "source": [
    "## Define Syllable Count Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0c2244f-c9bb-4cff-bebd-717dcb726202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syllable count function defined.\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate syllables in a word\n",
    "def syllable_count(word):\n",
    "    word = word.lower()\n",
    "    vowels = \"aeiouy\"\n",
    "    count = 0\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith(\"e\"):\n",
    "        count -= 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "print(\"Syllable count function defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b1ebfe-ff94-484a-a0b6-cb62d7425ed1",
   "metadata": {},
   "source": [
    "## Initialize Results List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c17eefe5-9a01-4262-b24b-3269ac7ba4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list for storing results\n",
    "results = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582f4a58-0842-46fd-b108-6f358251a7db",
   "metadata": {},
   "source": [
    "## Process Each Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0731ac70-7b79-493d-ba44-d8f083d3037b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed article 0001: Rising IT cities and its impact on the economy, en...\n",
      "Processed article 0002: Rising IT Cities and Their Impact on the Economy, ...\n",
      "Processed article 0003: Internet Demand’s Evolution, Communication Impact,...\n",
      "Processed article 0004: Rise of Cybercrime and its Effect in upcoming Futu...\n",
      "Processed article 0005: OTT platform and its impact on the entertainment i...\n",
      "Processed article 0006: The rise of the OTT platform and its impact on the...\n",
      "Processed article 0007: Rise of Cyber Crime and its Effects...\n",
      "Processed article 0008: Rise of Internet Demand and Its Impact on Communic...\n",
      "Processed article 0009: Rise of Cybercrime and its Effect by the Year 2040...\n",
      "Processed article 0010: Rise of Cybercrime and its Effect by the Year 2040...\n",
      "Processed article 0011: Rise of Internet Demand and its Impact on Communic...\n",
      "Processed article 0012: Rise of telemedicine and its Impact on Livelihood ...\n",
      "Processed article 0013: Rise of e-health and its impact on humans by the y...\n",
      "Processed article 0014: Rise of e-health and its impact on humans by the y...\n",
      "Processed article 0015: Rise of telemedicine and its Impact on Livelihood ...\n",
      "Processed article 0016: Rise of telemedicine and its Impact on Livelihood ...\n",
      "Processed article 0017: Rise of Chatbots and its impact on customer suppor...\n",
      "Processed article 0018: Rise of e-health and its impact on humans by the y...\n",
      "Processed article 0019: How does marketing influence businesses and consum...\n",
      "Processed article 0020: How advertisement increase your market value?...\n",
      "Processed article 0021: Negative effects of marketing on society...\n",
      "Processed article 0022: How advertisement/marketing affects business....\n",
      "Processed article 0023: Rising IT cities will impact the economy, environm...\n",
      "Processed article 0024: Rise of OTT platform and its impact on entertainme...\n",
      "Processed article 0025: Rise of Electric Vehicles and its Impact on Liveli...\n",
      "Processed article 0026: Rise of electric vehicle and its impact on livelih...\n",
      "Processed article 0027: Oil prices by the year 2040, and how it will impac...\n",
      "Processed article 0028: An outlook of healthcare by the year 2040, and how...\n",
      "Processed article 0029: AI in healthcare to Improve Patient Outcomes...\n",
      "Processed article 0030: What if the Creation is Taking Over the Creator?...\n",
      "Processed article 0031: What Jobs Will Robots Take From Humans in The Futu...\n",
      "Processed article 0032: Will Machine Replace The Human in the Future of Wo...\n",
      "Processed article 0033: Will AI Replace Us or Work With Us?...\n",
      "Processed article 0034: Will machine replace the human in the future of wo...\n",
      "Processed article 0035: How humans and machines are evolving to work toget...\n",
      "Processed article 0036: Error...\n",
      "Processed article 0037: How machine learning will affect your business?...\n",
      "Processed article 0038: Deep learning impact on areas of e-learning?...\n",
      "Processed article 0039: How to protect future data and its privacy?...\n",
      "Processed article 0040: How Machines, AI, Automations, and Robo-human are ...\n",
      "Processed article 0041: How Robo Human will Impact the Future?...\n",
      "Processed article 0042: How AI will change the World?...\n",
      "Processed article 0043: Future of Work: How AI Has Entered the Workplace...\n",
      "Processed article 0044: How machine learning used in finance and banking?...\n",
      "Processed article 0045: How AI will impact the future of work?...\n",
      "Processed article 0046: All you need to know about online marketing...\n",
      "Processed article 0047: Evolution of Advertising Industry...\n",
      "Processed article 0048: How Data Analytics can help your business respond ...\n",
      "Processed article 0049: Error...\n",
      "Processed article 0050: Environmental impact of the COVID-19 pandemic – Le...\n",
      "Processed article 0051: How Data Analytics and AI are used to halt the COV...\n",
      "Processed article 0052: What is the difference between Artificial Intellig...\n",
      "Processed article 0053: How Python became the first choice for Data Scienc...\n",
      "Processed article 0054: How Google fit measure heart and respiratory rates...\n",
      "Processed article 0055: What is the future of mobile apps?...\n",
      "Processed article 0056: Impact of AI in health and medicine...\n",
      "Processed article 0057: What patients like and dislike about telemedicine?...\n",
      "Processed article 0058: How we forecast future technologies?...\n",
      "Processed article 0059: Can robots tackle late-life loneliness?...\n",
      "Processed article 0060: Embedding care robots into society and practice: S...\n",
      "Processed article 0061: Management challenges for future digitalization of...\n",
      "Processed article 0062: Are we any closer to preventing a nuclear holocaus...\n",
      "Processed article 0063: Will technology eliminate the need for animal test...\n",
      "Processed article 0064: Will we ever understand the nature of consciousnes...\n",
      "Processed article 0065: Will we ever colonize outer space?...\n",
      "Processed article 0066: What is the chance Homo sapiens will survive for t...\n",
      "Processed article 0067: Why does your business need a chatbot?...\n",
      "Processed article 0068: How you lead a project or a team without any techn...\n",
      "Processed article 0069: Can You Be Great Leader Without Technical Expertis...\n",
      "Processed article 0070: How does artificial intelligence affect the enviro...\n",
      "Processed article 0071: How to Overcome Your Fear of Making Mistakes...\n",
      "Processed article 0072: Is Perfection the Greatest enemy of Productivity?...\n",
      "Processed article 0073: Global financial crisis 2008 causes/effects and it...\n",
      "Processed article 0074: Gender diversity and Equality in the tech industry...\n",
      "Processed article 0075: How to overcome your fear of making mistakes?...\n",
      "Processed article 0076: How Small Business can survive the Coronavirus Cri...\n",
      "Processed article 0077: Impacts of COVID 19 on Food products...\n",
      "Processed article 0078: Impacts of COVID 19 on Vegetable Vendors...\n",
      "Processed article 0079: Impact of COVID-19 pandemic on Tourism & Aviation ...\n",
      "Processed article 0080: Impact of COVID-19 pandemic on sports events aroun...\n",
      "Processed article 0081: Changing landscape and emerging trends in the Indi...\n",
      "Processed article 0082: Online gaming: Adolescent online gaming effects de...\n",
      "Processed article 0083: Human Rights Outlook...\n",
      "Processed article 0084: How Voice search makes your business a successful ...\n",
      "Processed article 0085: How the COVID-19 crisis is redefining jobs and ser...\n",
      "Processed article 0086: How to increase social media engagement for market...\n",
      "Processed article 0087: Impacts of COVID 19 on Streets Sides Food Stalls...\n",
      "Processed article 0088: Coronavirus impact on energy markets...\n",
      "Processed article 0089: COVID-19 Impact on Hospitality Industry...\n",
      "Processed article 0090: Lessons from the past: Some key learnings relevant...\n",
      "Processed article 0091: Estimating the impact of COVID-19 on the world of ...\n",
      "Processed article 0092: Estimating the impact of COVID-19 on the world of ...\n",
      "Processed article 0093: Travel and Tourism Outlook...\n",
      "Processed article 0094: Gaming Disorder and Effects of Gaming on Health....\n",
      "Processed article 0095: What is the repercussion of the environment due to...\n",
      "Processed article 0096: Due to the COVID-19 the repercussion of the enviro...\n",
      "Processed article 0097: Impact of COVID-19 pandemic on office space and co...\n",
      "Processed article 0098: Contribution of handicrafts (Visual Arts & Literat...\n",
      "Processed article 0099: How COVID-19 is impacting payment preferences?...\n",
      "Processed article 0100: How will COVID-19 affect the world of work?...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Process each article\n",
    "for filename in os.listdir(ARTICLES_FOLDER_PATH):\n",
    "    file_path = os.path.join(ARTICLES_FOLDER_PATH, filename)\n",
    "    url_id = filename.split('_')[0].replace('blackassign', '')\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        content = file.read()\n",
    "        heading = re.search(r'Heading: (.*)', content)\n",
    "        article_text = re.search(r'Article Text:\\n(.*)', content, re.DOTALL)\n",
    "        \n",
    "        if heading and article_text:\n",
    "            heading = heading.group(1)\n",
    "            article_text = article_text.group(1)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    print(f\"Processed article {url_id}: {heading[:50]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aba649c-c3f5-4d67-98ed-34c30e02d9c6",
   "metadata": {},
   "source": [
    "# here two files Processed article 0049: Error...\n",
    "# Processed article 0036: Error...\n",
    "### the articel will be not there "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9477eecb-824b-41aa-82e2-ec49703faa5d",
   "metadata": {},
   "source": [
    "## Clean and Tokenize the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd8a987a-5874-4b88-a264-2e5e6442ee73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized words: ['business', 'close', 'prevent', 'transmission', 'financial', 'concerns', 'job', 'losses', 'human', 'impacts']...\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Example processing for one article\n",
    "words = word_tokenize(article_text)\n",
    "words = [word for word in words if word.isalpha() and word.lower() not in stop_words]\n",
    "\n",
    "print(f\"Tokenized words: {words[:10]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f2a47f4-320c-45a7-a28d-70b3801b14ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            URL_ID                                                URL\n",
      "0  blackassign0001  https://insights.blackcoffer.com/rising-it-cit...\n",
      "1  blackassign0002  https://insights.blackcoffer.com/rising-it-cit...\n",
      "2  blackassign0003  https://insights.blackcoffer.com/internet-dema...\n",
      "3  blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...\n",
      "4  blackassign0005  https://insights.blackcoffer.com/ott-platform-...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to the input Excel file\n",
    "INPUT_EXCEL_PATH = r'C:\\Users\\dell\\OneDrive\\Sem5\\artilce\\Input.xlsx'\n",
    "\n",
    "# Read the input Excel file\n",
    "url_df = pd.read_excel(INPUT_EXCEL_PATH)\n",
    "\n",
    "print(url_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f7b223-f0a4-4e0b-98a4-b21018c3d598",
   "metadata": {},
   "source": [
    "## Calculate Derived Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec69e8c4-22bb-4d26-9579-54d755cf375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "# Process each article\n",
    "for filename in os.listdir(ARTICLES_FOLDER_PATH):\n",
    "    file_path = os.path.join(ARTICLES_FOLDER_PATH, filename)\n",
    "    url_id = filename.split('_')[0].replace('blackassign', '')\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        content = file.read()\n",
    "        heading = re.search(r'Heading: (.*)', content)\n",
    "        article_text = re.search(r'Article Text:\\n(.*)', content, re.DOTALL)\n",
    "        \n",
    "        if heading and article_text:\n",
    "            heading = heading.group(1)\n",
    "            article_text = article_text.group(1)\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Clean and tokenize the text\n",
    "        words = word_tokenize(article_text)\n",
    "        words = [word for word in words if word.isalpha() and word.lower() not in stop_words]\n",
    "\n",
    "        # Calculate derived variables\n",
    "        positive_score = sum(1 for word in words if word in positive_words)\n",
    "        negative_score = sum(1 for word in words if word in negative_words)\n",
    "        polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "        subjectivity_score = (positive_score + negative_score) / (len(words) + 0.000001)\n",
    "\n",
    "        sentences = sent_tokenize(article_text)\n",
    "        avg_sentence_length = len(words) / len(sentences)\n",
    "        complex_words = [word for word in words if syllable_count(word) > 2]\n",
    "        complex_word_count = len(complex_words)\n",
    "        percentage_of_complex_words = complex_word_count / len(words)\n",
    "        fog_index = 0.4 * (avg_sentence_length + percentage_of_complex_words)\n",
    "        avg_number_of_words_per_sentence = len(words) / len(sentences)\n",
    "        word_count = len(words)\n",
    "        syllable_per_word = sum(syllable_count(word) for word in words) / len(words)\n",
    "        personal_pronouns = len(re.findall(r'\\b(I|we|my|ours|us)\\b', article_text, re.I))\n",
    "        avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "\n",
    "        # Get the URL from the DataFrame\n",
    "        matching_url = url_df.loc[url_df['URL_ID'] == f'blackassign{url_id}', 'URL']\n",
    "        if not matching_url.empty:\n",
    "            current_url = matching_url.values[0]\n",
    "        else:\n",
    "            print(f\"URL_ID blackassign{url_id} not found in the DataFrame.\")\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        # Append the results to the list\n",
    "        results.append({\n",
    "            'URL_ID': f'blackassign{url_id}',\n",
    "            'URL': current_url,\n",
    "            'POSITIVE SCORE': positive_score,\n",
    "            'NEGATIVE SCORE': negative_score,\n",
    "            'POLARITY SCORE': polarity_score,\n",
    "            'SUBJECTIVITY SCORE': subjectivity_score,\n",
    "            'AVG SENTENCE LENGTH': avg_sentence_length,\n",
    "            'PERCENTAGE OF COMPLEX WORDS': percentage_of_complex_words,\n",
    "            'FOG INDEX': fog_index,\n",
    "            'AVG NUMBER OF WORDS PER SENTENCE': avg_number_of_words_per_sentence,\n",
    "            'COMPLEX WORD COUNT': complex_word_count,\n",
    "            'WORD COUNT': word_count,\n",
    "            'SYLLABLE PER WORD': syllable_per_word,\n",
    "            'PERSONAL PRONOUNS': personal_pronouns,\n",
    "            'AVG WORD LENGTH': avg_word_length\n",
    "        })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cf41a49-6941-4e42-b76d-4815db9cbb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to Output Data.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save the results to an Excel file\n",
    "results_df.to_excel(OUTPUT_FILE_PATH, index=False)\n",
    "print(f\"Results saved to {OUTPUT_FILE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b7b251-cbae-4fba-a432-a663b73e0c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
